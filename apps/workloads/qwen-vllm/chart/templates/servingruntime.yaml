apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: {{ .Values.runtime.name }}
  namespace: {{ .Values.namespace }}
spec:
  multiModel: false
  supportedModelFormats:
    - name: custom
      version: v1
      autoSelect: true
  protocolVersions:
    - v2
    - v1
  containers:
    - name: kserve-container
      image: {{ .Values.image.repository }}:{{ .Values.image.tag }}
      imagePullPolicy: {{ .Values.image.pullPolicy }}
      command:
        - python3
      args:
        - -m
        - vllm.entrypoints.openai.api_server
        - --host
        - {{ .Values.model.host | quote }}
        - --port
        - "{{ .Values.model.port }}"
        - --model
        - /mnt/models
        - --tensor-parallel-size
        - "{{ .Values.model.tensorParallelSize }}"
        - --dtype
        - {{ .Values.model.dtype }}
        - --gpu-memory-utilization
        - "{{ .Values.model.gpuMemoryUtilization }}"
        - --max-model-len
        - "{{ .Values.model.maxModelLen }}"
        - --trust-remote-code
      env:
        - name: VLLM_USE_ROCM
          value: "1"
      ports:
        - containerPort: {{ .Values.model.port }}
          name: http
      resources:
        limits:
{{ toYaml .Values.resources.limits | indent 10 }}
        requests:
{{ toYaml .Values.resources.requests | indent 10 }}
      livenessProbe:
        httpGet:
          path: /health
          port: http
        initialDelaySeconds: 30
      readinessProbe:
        httpGet:
          path: /health
          port: http
        initialDelaySeconds: 30
