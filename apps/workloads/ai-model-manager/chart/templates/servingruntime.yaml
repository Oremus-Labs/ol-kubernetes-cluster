apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: vllm-runtime
  namespace: {{ .Values.namespace }}
spec:
  multiModel: false
  supportedModelFormats:
    - name: custom
      version: v1
      autoSelect: true
  protocolVersions:
    - v2
    - v1
  containers:
    - name: kserve-container
      image: rocm/vllm-dev:nightly
      imagePullPolicy: IfNotPresent
      command:
        - /bin/sh
        - -c
      args:
        - |
          MODEL_PATH="${MODEL_DIR:-${MODEL_ID:-/mnt/models}}"
          exec python3 -m vllm.entrypoints.openai.api_server \
            --host "0.0.0.0" \
            --port "8080" \
            --model "${MODEL_PATH}" \
            "$@"
        - "model-manager-entrypoint"
      env:
        - name: VLLM_USE_ROCM
          value: "1"
      ports:
        - containerPort: 8080
          name: http
      livenessProbe:
        httpGet:
          path: /health
          port: http
        initialDelaySeconds: 30
        periodSeconds: 10
        failureThreshold: 12
      readinessProbe:
        httpGet:
          path: /health
          port: http
        initialDelaySeconds: 30
        periodSeconds: 10
