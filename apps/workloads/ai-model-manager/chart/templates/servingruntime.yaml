apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: vllm-runtime
  namespace: {{ .Values.namespace }}
spec:
  multiModel: false
  supportedModelFormats:
    - name: custom
      version: v1
      autoSelect: true
  protocolVersions:
    - v2
    - v1
  containers:
    - name: kserve-container
      image: "{{ .Values.runtime.image.repository }}:{{ .Values.runtime.image.tag }}"
      imagePullPolicy: {{ .Values.runtime.image.pullPolicy }}
      command:
        - /bin/sh
        - -c
      args:
        - |
          MODEL_PATH="${MODEL_DIR:-${MODEL_ID:-/mnt/models}}"
          if [ -n "${MODEL_ID:-}" ]; then
            CANDIDATE_DIR="${MODEL_PATH%/}"
            if [ ! -f "${CANDIDATE_DIR}/config.json" ]; then
              if [ -d "${CANDIDATE_DIR}/${MODEL_ID}" ] && [ -f "${CANDIDATE_DIR}/${MODEL_ID}/config.json" ]; then
                MODEL_PATH="${CANDIDATE_DIR}/${MODEL_ID}"
              fi
            fi
          fi
          exec python3 -m vllm.entrypoints.openai.api_server \
            --host "0.0.0.0" \
            --port "8080" \
            --model "${MODEL_PATH}" \
            "$@"
        - "model-manager-entrypoint"
      env:
        - name: VLLM_USE_ROCM
          value: "1"
      ports:
        - containerPort: 8080
          name: http
      livenessProbe:
        httpGet:
          path: /health
          port: http
        initialDelaySeconds: 30
        periodSeconds: 10
        failureThreshold: 12
      readinessProbe:
        httpGet:
          path: /health
          port: http
        initialDelaySeconds: 30
        periodSeconds: 10
